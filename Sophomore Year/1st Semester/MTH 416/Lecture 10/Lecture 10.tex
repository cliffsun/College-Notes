\documentclass{article}
\usepackage[left=2cm, right=2cm, top=1cm, bottom=1cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{pgfplots}

\lstset{
    language=C++,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue},
    commentstyle=\color{green},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    frame=single,
    breaklines=true,
    captionpos=b,
}


\title{MTH 416: Lecture 10}
\author{Cliff Sun}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{one minute paper}[theorem]{One Minute Paper}

\pagestyle{fancy}
\lhead{\textbf{\thepage}\ \ \nouppercase{\rightmark}}
\chead{MTH 416: Lecture 10}
\rhead{Cliff Sun}

\begin{document}

\maketitle

\section*{Lecture Span}
\begin{itemize}
    \item Rank-nullity Theorem
    \item Matrix Representation of Linear Transformations
\end{itemize}

\section*{Rank-nullity Theorem}

\begin{theorem}
    Suppose $T: \rightarrow W$, linear, V is finite dimensional, then:
    \begin{equation}
        \dim R(T) + \dim N(T) = \dim V
    \end{equation}
\end{theorem}

\subsection*{Example 1}

Let $T: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ and 
\begin{equation}
    T(x,y) = (y,x+y)
\end{equation}
Then 
\begin{equation}
    N(T) = \{(x,y) \in \mathbb{R}^2, (y,x+y = 0)\} \iff \{0\}
\end{equation}

Thus

\begin{equation}
    \dim R(T) + 0 = 2 \iff \dim R(T) = 2
\end{equation}

This means that $R(T)$ is surjective, and covers all of $\mathbb{R}^2$, thus $T$ is surjective. 

\subsection*{Example 2}

\begin{equation}
    T: P_n(\mathbb{R}) \rightarrow P_n(|\mathbb{R})
\end{equation}

\begin{equation}
    T = f'
\end{equation}

$N(T) = \{c\}: c \in \mathbb{R} \iff P_0(\mathbb{R}) \rightarrow P_n(\mathbb{R})$

So $\dim N(T) = 1$. By rank nullity, we have that 

\begin{equation}
    \dim R(T) + \dim N(T) = \dim P_n(\mathbb{R})
\end{equation}

Then $R(T) = P_{n-1}(\mathbb{R})$ and has dimension $n$. 

\begin{corollary}
    Suppose $T: V \rightarrow W$, where $\dim V = m$ and $\dim W = n$ then 
    \begin{enumerate}
        \item If $T$ is injective, then $n \geq m$
        \item If $T$ is surjective, then $m \geq n$
        \item If $n = m$, then $T$ is injective iff it's surjective. 
    \end{enumerate}
\end{corollary}

Let's prove (3).

\begin{proof}
    Suppose that $\dim V = \dim W$, then we know that 
    \begin{equation}
        T \text{ injective } \iff N(T) = \{0\}
    \end{equation}
    But 
    \begin{equation}
        N(T) = \{0\} \iff \dim N(T) = 0
    \end{equation}
    Thus 
    \begin{equation}
        \dim R(T) + \dim N(T) = n \iff \dim R(T) = n
    \end{equation}
    \begin{equation}
        R(T) = W
    \end{equation}
    This is equivalent to saying that $R(T)$ is surjective. 
\end{proof}

\section*{Matrix Representation of Linear Transformations}

Suppose $V$ is a vector space, with basis $\beta = \{v_1, \dots, v_n\}$. Then say $v \in V$ can be written \underline{uniquely} such that 
\begin{equation}
    v = a_1v_1 + \dots + a_nv_n
\end{equation}

\begin{definition}
    The \underline{coordinate vector} of $v$ with respect to the basis $\beta$ is
    \begin{equation}
        [v]_\beta \iff <a_1,a_2,\dots,a_n> \in \mathbb{R}^n
    \end{equation}
\end{definition}

\subsection*{Example 1}
Suppose $V = P_2(\mathbb{R})$ with basis $\{1,x,x^2\}$ and $v = x^2 + 2x + 3$. Then 
\begin{equation}
    [v]_\beta = <3,2,1>
\end{equation}
Note, the order of the basis matters. If $\beta' = <x^2,x,1>$, then the coordinate vector would be 
\begin{equation}
    [v]_\beta = <1,2,3>
\end{equation}
In other words, coordinate vectors depend on an \underline{ordered basis}. 

\subsection*{Example 2}

Let $V = \mathbb{R}^n$ with the standard ordered basis, that is 
\begin{equation}
    \beta = <e_1,e_2, \dots, e_n> = \{\{1,0,\dots,0\}, \{0,1,\dots,0\}, \dots, \}
\end{equation}

Then for any $v = <v_1,\dots,v_n> \in \mathbb{R}^n$, we have that 
\begin{equation}
    [v]_\beta = v
\end{equation}

\begin{theorem}
    If $V$ has an ordered basis $\beta = \{v_1, \dots, v_n\}$, then the function 
    \begin{equation}
        T: V \rightarrow \mathbb{R}^n
    \end{equation}
    and $T(v) = [v]_\beta$. This is a bijective linear transformation. That is, each vector is mapped to its coordinate vector.
    This should be bijective because each vector in $V$ can be expressed uniquely as the sum of its basis vectors. 
\end{theorem}

Let $V$ and $W$ be vector spaces, and suppose we have ordered basis 
\begin{equation}
    \beta = \{v_1,\dots,v_n\} \in V
\end{equation}
\begin{equation}
    \gamma = \{w_1,\dots,w_m\} \in W
\end{equation}

Suppose $T \rightarrow W$ is linear. Recall $T$ is uniquely determined by $T(v_1), \dots, T(v_n)$.

\begin{definition}
    A matrix of $T$ in the ordered basis $\beta$, $\gamma$ is 
    \begin{equation}
        [T]_\beta^\gamma = \begin{pmatrix}
            [T(v_1)]_\gamma & \dots & [T(v_n)]_\gamma
        \end{pmatrix}
    \end{equation}
    This is an $m \times n$ matrix. Explicitly, if the entry in position $(i,j)$ is $a_{ij}$, then 
    \begin{equation}
        T(v_j) = a_{1j}w_1 + \dots + a_{mj}w_m
    \end{equation}
\end{definition}

\subsection*{Example 1}

$T: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ such that 
\begin{equation}
    T(x,y) = (x+y,y)
\end{equation}
\begin{equation}
    \beta = \{e_1,e_2\}
\end{equation}
\begin{equation}
    [T]_\beta^\gamma = \begin{pmatrix}
        [T(v_1)]_\gamma & \dots & [T(v_n)]_\gamma
    \end{pmatrix}
\end{equation}
or 
\begin{equation}
    [T]_\beta^\gamma = \begin{pmatrix}
        1 & 1 \\
        0 & 1
    \end{pmatrix}
\end{equation}

Because 
\begin{equation}
    T(<1,0>) = <x+y, y> \iff <1, 0> 
\end{equation}
and 
\begin{equation}
    T(<0,1>) = <x+y,y> \iff <0 + 1, 1> \iff <1,1>
\end{equation}

Let 
\begin{equation}
    \gamma = \{\begin{pmatrix}
        1 \\
        0
    \end{pmatrix}, \begin{pmatrix}
        1 \\ 
        1
    \end{pmatrix}\}
\end{equation}

Fact: Once we choose the ordered basis $\beta$ for $V$, and $\gamma$ for $W$, then
\begin{equation}
    T \iff [T]_\beta^\gamma
\end{equation}
is a one-to-one correspondence between linear Transformations and $m \times n$ matrices. Suppose 
\begin{equation}
     A \in M_{m \times n}(\mathbb{R})
\end{equation}
and 
\begin{equation}
    x = \begin{pmatrix}
        x_1 \\
        \dots \\
        x_n
    \end{pmatrix} \in \mathbb{R}^n
\end{equation}

Recall $Ax$ is the vector $\gamma \in \mathbb{R}^m$ such that 
\begin{equation}
    y_i = \sum_{j = i}^{n}A_{ij}x_j
\end{equation}

Note: if $u_1, \dots, u_n$ are columns of $A$, then 
\begin{equation}
    Ax = x_1u_1 + x_2u_2 + \dots + x_nu_n
\end{equation}

Note: $LS(A,b) \iff Ax = b$ such that $x$ is a vector of variables. 

\begin{theorem}
    Suppose $T: V \rightarrow W$ is linear, and $\beta$ and $\gamma$ are ordered basis for $V$ and $W$ respectively. Then for any $v \in V$, we have that 
    \begin{equation}
        [T(v)]_\gamma = [T]_\beta^\gamma [v]_\beta
    \end{equation}
    That is, every vector in $T(v)$ can be written as a matrix multiplication of $[v]_\beta$. Idk why this notation is a thing, since it's basically saying
    \begin{equation}
        w = Av : \forall v \in V \land w \in W
    \end{equation}
\end{theorem}

\end{document}