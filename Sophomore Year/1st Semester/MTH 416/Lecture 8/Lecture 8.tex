\documentclass{article}
\usepackage[left=2cm, right=2cm, top=1cm, bottom=1cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{pgfplots}

\lstset{
    language=C++,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue},
    commentstyle=\color{green},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    frame=single,
    breaklines=true,
    captionpos=b,
}


\title{MTH 416: Lecture 8}
\author{Cliff Sun}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{one minute paper}[theorem]{One Minute Paper}

\pagestyle{fancy}
\lhead{\textbf{\thepage}\ \ \nouppercase{\rightmark}}
\chead{MTH 416: Lecture 8}
\rhead{Cliff Sun}

\begin{document}

\maketitle

\section*{Lecture Span}
\begin{itemize}
    \item Finding basis for spans and nullspaces
    \item Linear transformations
\end{itemize}

\section*{Finding bases for spans \& nullspaces}

Given vectors $u_1, \dots, u_k \in \mathbb{R}^n$. We can form the matrix $A$ ($n \times k$)

\[\begin{pmatrix}
    u_1 & u_2 & \dots & u_k
\end{pmatrix}\]

This coudl lead to 2 types of subspaces:

\begin{enumerate}
    \item span($\{u_1, \dots, u_k\}$) $\subseteq \mathbb{R}^n$ (this is called the \underline{column space} of A)
    \item The \underline{nullspace} of a matrix $A$ is 
    \begin{equation}
        N(A) = \{(x_1, \dots, x_k) \in \mathbb{R}^k : x_1u_1 + \dots + x_ku_k = 0\}
    \end{equation} 
    It consists of linearly dependencies among $u$. $N(A) = \vec{0}$, iff $u_1, \dots, u_k$ is linearly independent. 
\end{enumerate}

\subsection*{How to find the basis for $N(A)$}

Ex:

\[\left[\begin{array}{cccc|c}
    1 & 2 & 2 & -1 & 0 \\
    3 & 6 & 4 & -1 & 0 \\
    -1 & -2 & 1 & -2 & 0
\end{array}\right]\]

\begin{equation}
    R_2 = R_2 - 3R_1
\end{equation}

\[\left[\begin{array}{cccc|c}
    1 & 2 & 2 & -1 & 0 \\
    0 & 0 & -2 & 2 & 0 \\
    -1 & -2 & 1 & -2 & 0
\end{array}\right]\]

\begin{equation}
    R_3 = R_3 + R_1
\end{equation}

\[\left[\begin{array}{cccc|c}
    1 & 2 & 2 & -1 & 0 \\
    0 & 0 & -2 & 2 & 0 \\
    0 & 0 & 3 & -3 & 0
\end{array}\right]\]

\begin{equation}
    R_2 = R_2/2
\end{equation}
\begin{equation}
    R_3 = R_3 + 3R_2
\end{equation}

\[\left[\begin{array}{cccc|c}
    1 & 2 & 2 & -1 & 0 \\
    0 & 0 & 1 & -1 & 0 \\
    0 & 0 & 0 & 0 & 0
\end{array}\right]\]

\begin{equation}
    R_1 = R_1 - 2R_2
\end{equation}

\[\left[\begin{array}{cccc|c}
    1 & 2 & 0 & 1 & 0 \\
    0 & 0 & 1 & -1 & 0 \\
    0 & 0 & 0 & 0 & 0
\end{array}\right]\]

\textbf{Fact:} We didn't change the nullspace of this matrix! Since the nullspace is a solution set to $A$, and elementary row operations don't change the solution set.

\subsubsection*{What are the solutions?}

Note, $a_2, a_4$ are free variables:

\begin{equation}
    a_1 = -2a_2 - a_4
\end{equation}
\begin{equation}
    a_3 = a_4
\end{equation}

General solution tells us:

\begin{equation}
    N(A) = \{(-2a_2-a_4, a_2, a_4, a_4) : a_2, a_3 \in \mathbb{R}\}
\end{equation}

But let's write the basis of this $N(A)$,
\begin{equation}
    (-2a_2-a_4, a_2, a_4, a_4) \iff a_2(-2,1,0,0) + a_4(-1,0,1,1)
\end{equation}

Then

\begin{equation}
    span\{(-2,1,0,0), (-1,0,1,1)\} = N(A)
\end{equation}

They are linearly independent, thus, they form a basis over $N(A)$. Note, they will always be linearly independent, since $a_i$ will always represent 
exactly one variable, and no two $a_i,a_k$ can be one variable, thus these degrees of freedom prevent linear dependence. 

\subsubsection*{General rule}

\begin{enumerate}
    \item Row reducing doesn't change the nullspace
    \item if $A$ is in RREF, then $dim(N(A)) = \#(\text{ free variables })$ = \#(columns) - \#(pivots)
    \item To find the basis for $N(A)$, set one free variable to $1$ and the rest to 0, and repeat for each free variable. 
\end{enumerate}

\subsection*{Methods to find basis vectors}

Form a matrix whose \underline{rows} are $u_i$. In this case, the \underline{row space} is relevant to our problem. 

\textbf{Fact:} Row reducing a matrix doesn't change its rowspace. 

\[\left[\begin{array}{ccc}
    1 & 3 & -1 \\
    2 & 6 & -2 \\
    2 & 4 & 1 \\
    -1 & -1 & -2
\end{array}\right]\]

The answer is 

\[\left[\begin{array}{ccc}
    1 & 0 & \frac{7}{2} \\
    0 & 1 & -\frac{3}{2} \\
    0 & 0 & 0 \\
    0 & 0 & 0
\end{array}\right]\]

Note:

\begin{equation}
    span(\{u_1,u_2,u_3,u_4\}) = span(\{(1,0,\frac{7}{2}), (0,1,-\frac{3}{2})\})
\end{equation}

In fact, these vectors are linearly independent. 

\subsubsection*{General rule}

To find a basis for the row space of a matrix, just row reduce it and choose all the non-zero rows or rows with pivots. 

\textbf{Recall:} Given a set of vectors $\{u_1, \dots, u_k\}$, it's always possible to find a basis for $span(\{u_1, \dots, u_k\})$ consisting of a subset of the $u_i$. Basically, 
destroy all linearly dependent vectors from the set until all vectors are linearly independent. 

\[\left[\begin{array}{cccc}
    1 & 2 & 2 & -1 \\
    3 & 6 & 4 & -1 \\
    -1 & -2 & 1 & -2
\end{array}\right]\]

Answer:

\[\left[\begin{array}{cccc}
    1 & 2 & 0 & 1\\
    0 & 0 & 1 & -1\\
    0 & 0 & 0 & 0
\end{array}\right]\]

\textbf{Warning:} Row reducing does change the column space!

\textbf{However:} It doesn't change the linear dependencies amongst the columns! (Aka the nullspace)

In particular, 

\begin{equation}
    2C_1 = C_2
\end{equation}
\begin{equation}
    C_4 = C_1 - C_3
\end{equation}

In RREF, $C_1 \; \& \; C_3$ are linearly independent. And all the other columns are linear combinations of these linearly independent columns. Therefore, the same is true for the previous columns. 
Therfore, $\{u_1,u_3\}$ is the basis for $span(\{u_1,u_2,u_3,u_4\})$. 

\subsubsection*{General Rule}

To find a basis for $span(\{u_1,\dots, u_k\})$, which is a subset of $\{u_1, \dots, u_k\}$, row reduce the matrix, then choose the columns associated with the pivots. 

\section*{Linear Transformations}

\textbf{General Philosophy:} to understand any types of objects, also good to understand the functions between two types of objects. Ex:
to understand sets, you must study functions (bijections, injective functions, etc.). 

\subsubsection*{Example}

In calculus, to understand $\mathbb{R}$, you must understand functions $f: \mathbb{R} \rightarrow \mathbb{R}$. But only really continuous functions. 

\subsubsection*{Question:} Given objects with some type of structure, what functions between them preserve the structure. Linear functions? 

In linear algebra, the "objects" are vector spaces, the "structure" is addition and scalar multiplication, functions that respect that structure are "Linear transformations".

\begin{definition}
    Given 2 vector spaces $V$ and $W$, (both over $\mathbb{R}$), a linear transformation from $V \rightarrow W$ would be a function $T: V \rightarrow W$ must satisfy both rules. 
    \begin{enumerate}
        \item $T(v_1 + v_2) = Tv_1 + T_2$ for all $v_1, v_2 \in V$
        \item $T(cv) = cT(v)$ for all $v \in V$ and $c \in \mathbb{R}$. 
    \end{enumerate}
\end{definition}

\subsection*{Example 1.} 
\begin{equation}
    T: \mathbb{R}^2 \rightarrow \mathbb{R}^2
\end{equation}
\begin{equation}
    T(x,y) = (-y,x)
\end{equation}

A clockwise rotation of 90 degrees. For all $v_1 = (x_1,y_1)$ and $v_2 = (x_2,y_2)$ and $c \in \mathbb{R}$
\begin{enumerate}
    \item $T(v_1+v_2) = T(x_1+x_2, y_1+y_2) \iff (-(y_1+y_2), x_1 + x_2) = (-y_1,x_1) + (-y_2, x_2)  \iff T(v_1) + T(v_2)$
    \item $T(cv) = T(cx,xy) \iff (-cy,cx) \iff c(-y,x) \iff cT(v)$
\end{enumerate}

So this is a linear transformation. 

\subsection*{Example 2.}

\begin{equation}
    T: \mathbb{R}^3 \rightarrow \mathbb{R}^2
\end{equation}
\begin{equation}
    T(x,y,z) = (x,y)
\end{equation}

"Flattens" 3d space onto 2d. This is also a linear transformations. 

\subsection*{Example 3.}

\begin{equation}
    T: V \rightarrow W
\end{equation}
\begin{equation}
    T(v) = 0 \; \forall v
\end{equation}

\subsection*{Example 4.}

\begin{equation}
    T: V \rightarrow V
\end{equation}

\begin{equation}
    T(v) = v
\end{equation}

Is also a linear transformation. 

\subsection*{Example 5.}

\begin{equation}
    T: P(\mathbb{R}) \rightarrow P(\mathbb{R})
\end{equation}
\begin{equation}
    T(f) = f'
\end{equation}
\begin{equation}
    (f+g)' = f' + g'
\end{equation}
\begin{equation}
    (cf)' = c(f)'
\end{equation}

This is also a linear transformation. 

\end{document}