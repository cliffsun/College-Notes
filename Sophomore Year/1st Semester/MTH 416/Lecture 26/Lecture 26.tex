\documentclass{article}
\usepackage[left=2cm, right=2cm, top=2cm, bottom=2cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{pgfplots}

\lstset{
    language=C++,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue},
    commentstyle=\color{green},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    frame=single,
    breaklines=true,
    captionpos=b,
}


\title{MTH 416: Lecture 26}
\author{Cliff Sun}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{one minute paper}[theorem]{One Minute Paper}

\pagestyle{fancy}
\lhead{\textbf{\thepage}\ \ \nouppercase{\rightmark}}
\chead{MTH 416: Lecture 26}
\rhead{Cliff Sun}

\begin{document}

\maketitle

\section*{Lecture Span}
\begin{itemize}
    \item Adjoint, Least squares 
    \item Jordan Canonical Form
\end{itemize}

\begin{lemma}
    Let $F = \mathbb{R}, \mathbb{C}$, and $A \in M_{m \times n}(\mathbb{F})$. For any column vectors $x \in F^n$ and $y \in F^m$. Then we have that 
    \begin{equation}
        \langle Ax, y \rangle_m = \langle x, A^* y\rangle_n
    \end{equation}
    Note: 
    \begin{equation}
        A^* \equiv \overline{A^T}
    \end{equation}
\end{lemma}

\begin{definition}
    Suppose $T: V \rightarrow W$, where $V, W$ are inner product spaces. An \underline{adjoint} of $T$ is a linear transformation 
    $T^*: W \rightarrow V$ such that for all $v \in V$ and $w \in W$, we have 
    \begin{equation}
        \langle T(v), w \rangle_W = \langle v, T^*(w) \rangle_V
    \end{equation} 
\end{definition}

\begin{theorem}
    If $T$ is a linear transformation between 2 finite dimensional vector spaces, then it has a unique adjoint. 
\end{theorem}

\textbf{Proof Idea:} Fix $w \in W$, then we want to define $T^*(w)$. The left hand side of equation 3 is a linear transformation 
\begin{equation}
    f: V \rightarrow F
\end{equation}
\begin{equation}
    f(v) = \langle T(v), w \rangle_W
\end{equation}
\underline{Fact:} Every linear transformation $f: V \rightarrow F$ can be expressed as $\langle v, x \rangle_V$ for some unique $x \in V$. Define $T^*(w) = x$.
This then proves the theorem.  

\underline{Fact:} If we are given an orthonormal basis $\beta$ for $V$ and $\gamma$ for $W$, then 
\begin{equation}
    [T^*]_\gamma^\beta = \overline{\left([T]_\beta^\gamma\right)}^T
\end{equation}

\begin{lemma}
    Suppose $A \in M_{m\times n}(F)$ has rank $n$. Then the $n \times n$ matrix $A^* A$ is invertible. 
\end{lemma}

\begin{proof}
    Let $A$ be given, apply rank nullity to $L_A : F^n \rightarrow F^m$, then 
    \begin{equation}
        \dim(F^n) = rank(A) + null(A)
    \end{equation}
    \begin{equation}
        \iff n = n + 0
    \end{equation}
    So 
    \begin{equation}
        N(A) = \{0\}
    \end{equation}
    We now prove that $N(A^* A) = \{0\} \iff $ $A^* A$ is invertible. We claim that 
    \begin{equation}
        A^* A x = 0 \implies x = 0
    \end{equation}
    \begin{proof}
        Suppose $x \in F^n$ and $A^* A x = 0$, then
        \begin{equation}
            0 = \langle A^* Ax, x \rangle_n = \langle Ax, (A^*)^* x \rangle_m = \langle Ax, Ax \rangle_m = Ax = 0
        \end{equation}
    \end{proof}
    So $Ax = 0$, thus it follows that $x = 0$. So $N(A^* A) = \{0\}$ which means that $A^* A$ is invertible. 
\end{proof}

This has applications in least squares, recall that we're given $n$ data points $(x_i, y_i) \in \mathbb{R}^2$. We want the 'best fit' line $y = mx + b$. We convert this into 
vectors 
\begin{equation}
    x = \begin{pmatrix}
        x_1 \\
        x_2 \\
        \dots
    \end{pmatrix} \land y = \begin{pmatrix}
        y_1 \\
        y_2 \\
        \dots
    \end{pmatrix} \land u = \begin{pmatrix}
        1 \\
        1 \\
        \dots \\
    \end{pmatrix}
\end{equation}

Then the least square parameters are given by 
\begin{equation}
    proj_W(y) = mx + bu
\end{equation}
Where $W = span(\{x,u\})$. We convert this into matrices:
\begin{equation}
    A = \begin{pmatrix}
        x_1 & 1 \\
        \dots & \dots
    \end{pmatrix} \in M_{n \times 2}(\mathbb{R})
\end{equation}
We define
\begin{equation}
    v = \begin{pmatrix}
        m \\
        b 
    \end{pmatrix} \in \mathbb{R}^2
\end{equation}
So 
\begin{equation}
    Av = mx + bu
\end{equation}
We must find $v_0 = \begin{pmatrix}
    m_0 \\
    b_0
\end{pmatrix} \in \mathbb{R}^2$ such that 
\begin{equation}
    Av_0 = proj_W(y)
\end{equation}
To minimize the cost, we need that 
\begin{equation}
    Av_0 - y \in W^{\perp}
\end{equation}
\begin{equation}
    \iff \langle Av_0, Av_0 - y\rangle = 0 
\end{equation}
Then 
\begin{equation}
    \iff \langle v_0, A^* (A v_0 - y) \rangle
\end{equation}
This can only happen is 
\begin{equation}
    A^* (Av_0 - y) = 0
\end{equation}
We multiply it out 
\begin{equation}
    A^* A v_0 = A^* y
\end{equation}
Because $A^* A$ is invertible, we have that 
\begin{equation}
    v_0 =  (A^* A)^{-1} A^*y
\end{equation}
But we need to know whether
\begin{equation}
    rank(A) = 2
\end{equation}
This is always true unless all $(x_i, y_i)$ have the same $x_i$ value. 

\section*{Jordan Canonical Form}

Recall: if $A \in M_{n \times n}(F)$, then there are 2 things that prevent $A$ from being diagonalizable. 
\begin{enumerate}
    \item Not enough eigenvalues 
    \item Not enough eigenvectors (for some $\lambda_i$, geo multi (dimension of eigen space) $<$ alg multi (degree of polynomial associated with $\lambda_i$))
\end{enumerate}
If $F = \mathbb{C}$, then $1.$ cannot happen by the fundamental theorem of Algebra.
\begin{center}
    \textbf{Question:} If $A$ is a matrix with problem $(2)$, how close can we get to a diagonalizing it? 
\end{center}

Is there soem form of "almost diagonal" matrix such that every $A$ is similar to some matrixx of the given form. Yes, this is called the \underline{Jordan canonical form}. 

\begin{definition}
    \begin{enumerate}
        \item A \underline{Jordan block} is a $k \times k$ (for $k \geq 1)$ matrix of the form 
        \begin{equation}
            \begin{pmatrix}
                \lambda_i & 1 & 0 & 0 \dots \\
                0 & \lambda_i & 1 & 0 \dots \\
                \dots 
            \end{pmatrix}
        \end{equation}
        All the $\lambda_i$'s are the same. 
        \item A matrix $B$ is in \underline{Jordan Canonical Form} if it has the form 
        \begin{equation}
            \begin{pmatrix}
                B_1 & 0 \dots \\
                0 & B_2 & \dots \\
                \dots 
            \end{pmatrix}
        \end{equation}
        Where $B_i$ is a Jordan block. Note: since we allow $1 \times 1$ Jordan blocks, all diagonal matrices are in JCF.  
    \end{enumerate}
\end{definition}
\begin{center}
    \textbf{Claim:} A Jordan block of size $k \geq 1$ is not diagonalizable. 
\end{center}
\begin{proof}
    Let $A$ be a $k\times k$ Jordan block. Then 
    \begin{equation}
        \det(A - tI) = (\lambda - t)^k
    \end{equation}
    So the only eigenvalue is $\lambda$ with alg multi = $k$. We note that
    \begin{center}
        geo multi = 1
    \end{center}
\end{proof}

More generally, a JCF matrix $B$ is diagonalizable iff all blocks are $1 \times 1$. Which iff it is already diagonal....

\begin{theorem}
    Let $A \in M_{n \times n}(F)$. A is similar to a matrix in JCF if and only if char poly(A) splits over $F$. 
\end{theorem}
\end{document}